{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory-efficient SAGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyximport; pyximport.install()\n",
    "import saga_authors\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "import sklearn.linear_model, sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Problem: Prediction of the release year of a song from audio features. Songs are mostly western, commercial tracks ranging from 1922 to 2011, with a peak in the year 2000s.\n",
    "\n",
    "* Dataset can be downloaded at https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD.\n",
    "* 90 audio attributes: 12 = timbre average, 78 = timbre covariance\n",
    "    * Features extracted from the 'timbre' features from The Echo Nest API. We take the average and covariance over all 'segments', each segment being described by a 12-dimensional timbre vector.\n",
    "* The first value is the year (target), ranging from 1922 to 2011. \n",
    "* train: first 463,715 examples, test: last 51,630 examples\n",
    "    * It avoids the 'producer effect' by making sure no song from a given artist ends up in both the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('../YearPredictionMSD.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Song release year.\n",
    "train_target = data[:463715,0]  #.astype(np.int32)\n",
    "test_target  = data[463715:,0]  #.astype(np.int32)\n",
    "assert test_target.shape == (51630,)\n",
    "\n",
    "# Audio features.\n",
    "train_data = data[:463715,1:]\n",
    "test_data  = data[463715:,1:]\n",
    "assert train_data.shape == (463715,90)\n",
    "assert test_data.shape == (51630,90)\n",
    "\n",
    "# Subset for fast testing purpose.\n",
    "#n = 10000\n",
    "#train_data = data[:n,1:]\n",
    "#train_target = data[:n,0]\n",
    "\n",
    "# Normalize so as to do not need an intercept.\n",
    "intercept = train_target.mean()\n",
    "data_mean = train_data.mean(axis=0)\n",
    "train_target = train_target - intercept\n",
    "test_target = test_target - intercept\n",
    "train_data = train_data - data_mean\n",
    "test_data = test_data - data_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression: ground truth\n",
    "\n",
    "* Sanity check of the performance of a linear classifier.\n",
    "* Exact least-square solution to compare with SAGA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(x, dataset, plot=None):\n",
    "    A = eval('{}_data'.format(dataset))\n",
    "    y = eval('{}_target'.format(dataset)) + intercept\n",
    "    pred = A.dot(x) + intercept\n",
    "    score = sklearn.metrics.r2_score(y, pred)\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(17,5))\n",
    "        plt.plot(pred[:plot], '.', label='predicted')\n",
    "        plt.plot(y[:plot], '.', label='ground truth')\n",
    "        plt.title(dataset)\n",
    "        plt.xlabel('sample')\n",
    "        plt.ylabel('release year')\n",
    "        plt.legend()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_natural = np.linalg.lstsq(train_data, train_target)[0]\n",
    "print('R^2 score: {:.4f}'.format(score(x_natural, 'train', plot=2000)))\n",
    "print('R^2 score: {:.4f}'.format(score(x_natural, 'test', plot=2000)))\n",
    "\n",
    "cls = sklearn.linear_model.LinearRegression(fit_intercept=True)\n",
    "cls.fit(train_data, train_target)\n",
    "assert cls.intercept_ < 1e-10\n",
    "np.testing.assert_allclose(cls.coef_, x_natural)\n",
    "\n",
    "cls = sklearn.linear_model.Ridge(alpha=1e7, fit_intercept=True)\n",
    "cls.fit(train_data, train_target)\n",
    "print('R^2 score: {:.4f}'.format(score(cls.coef_, 'train')))\n",
    "print('R^2 score: {:.4f}'.format(score(cls.coef_, 'test', plot=2000)))\n",
    "\n",
    "cls = sklearn.linear_model.Lasso(alpha=1e0, fit_intercept=True)\n",
    "cls.fit(train_data, train_target)\n",
    "print('R^2 score: {:.4f}'.format(score(cls.coef_, 'train')))\n",
    "print('R^2 score: {:.4f}'.format(score(cls.coef_, 'test', plot=2000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAGA\n",
    "\n",
    "Various implementations.\n",
    "\n",
    "1. Cython code from the authors.\n",
    "    * To solve least-square problems only.\n",
    "    * It uses the tricks from Section 4 of the paper.\n",
    "2. Minimal and straightforward implementation by us.\n",
    "    * It provides the same results and is only 3 times slower.\n",
    "3. Mini-batch version.\n",
    "    1. Mini-batches are picked at random during initialization and stay the same for the whole training.\n",
    "    1. Mini-batches are picked at random for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def saga_lstsq_authors(A, y, maxiter, gamma, reg=0):\n",
    "    \"\"\"\n",
    "    Solve min_x ||Ax - b||_2^2 reg*||x||_2^2\n",
    "    \n",
    "    Parameters:\n",
    "        gamma: step size or learning rate\n",
    "        reg:   amount of L2 regularization\n",
    "    \"\"\"\n",
    "    # Proper data type.\n",
    "    A = sparse.csc_matrix(A.T)\n",
    "    A.indices = A.indices.astype(np.int64)\n",
    "    A.indptr = A.indptr.astype(np.int64)\n",
    "    y = y.astype(np.float64)\n",
    "    \n",
    "    # Algorithm (eta is the inverse step size).\n",
    "    props = {'eta': 1 / gamma, 'reg': reg}\n",
    "    x = saga_authors.saga_lstsq(A, y, maxiter, props)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saga(A, y, gradf, prox, maxiter, gamma):\n",
    "    \"\"\"Minimal and straightforward implementation of SAGA.\"\"\"\n",
    "    n, d = A.shape  # number of observations x dimensionality\n",
    "    x = np.zeros(d)\n",
    "    grads = np.array([gradf(A, x, y, i) for i in range(n)])\n",
    "    avg = grads.mean(axis=0)\n",
    "    X = np.empty((maxiter, d))\n",
    "    \n",
    "    for epoch in range(maxiter):\n",
    "        #indices = np.random.permutation(n)\n",
    "        indices = np.random.randint(0, n, n)\n",
    "        \n",
    "        for i in indices:\n",
    "            grad = gradf(A, x, y, i)\n",
    "            x = x - gamma * (grad - grads[i,:] + avg)\n",
    "            x = prox(x, gamma)\n",
    "            avg += (grad - grads[i,:]) / n\n",
    "            grads[i,:] = grad\n",
    "            \n",
    "        X[epoch,:] = x\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saga_mb(A, y, gradf, prox, maxiter, gamma, mb_size=100):\n",
    "    \"\"\"\n",
    "    Mini-batch version of SAGA.\n",
    "    Mini-batches are chosen at random and fixed for the whole training.\n",
    "    \"\"\"\n",
    "    n, d = A.shape  # number of observations x dimensionality\n",
    "    n //= mb_size\n",
    "    x = np.zeros(d)\n",
    "    X = np.empty((maxiter, d))\n",
    "    \n",
    "    batches = np.random.permutation(n*mb_size).reshape((mb_size, -1))\n",
    "    grads = np.array([gradf(A, x, y, batches[:,i]) for i in range(n)])\n",
    "    avg = grads.mean(axis=0)\n",
    "    \n",
    "    for epoch in range(maxiter):\n",
    "        \n",
    "        for i in np.random.randint(0, n, n):\n",
    "            grad = gradf(A, x, y, batches[:,i])\n",
    "            x = x - gamma * (grad - grads[i,:] + avg)\n",
    "            x = prox(x, gamma)\n",
    "            avg += (grad - grads[i,:]) / n\n",
    "            grads[i,:] = grad\n",
    "            \n",
    "        X[epoch,:] = x\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saga_mb_2(A, y, gradf, prox, maxiter, gamma, mb_size=100):\n",
    "    \"\"\"\n",
    "    Mini-batch version of SAGA.\n",
    "    Mini-batches are chosen at random for each epoch.\n",
    "    \"\"\"\n",
    "    n, d = A.shape  # number of observations x dimensionality\n",
    "    n //= mb_size\n",
    "    x = np.zeros(d)\n",
    "    X = np.empty((maxiter, d))\n",
    "    \n",
    "    for epoch in range(maxiter):\n",
    "        \n",
    "        batches = np.random.permutation(n*mb_size).reshape((mb_size, -1))\n",
    "        grads = np.array([gradf(A, x, y, batches[:,i]) for i in range(n)])\n",
    "        avg = grads.mean(axis=0)\n",
    "        \n",
    "        for i in np.random.randint(0, n, n):\n",
    "            grad = gradf(A, x, y, batches[:,i])\n",
    "            x = x - gamma * (grad - grads[i,:] + avg)\n",
    "            x = prox(x, gamma)\n",
    "            avg += (grad - grads[i,:]) / n\n",
    "            grads[i,:] = grad\n",
    "            \n",
    "        X[epoch,:] = x\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic problem\n",
    "\n",
    "Test the methods and be sure that they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_convergence(f, A, y, Xs, x_natural, labels):\n",
    "    plt.figure(figsize=(17,5))\n",
    "    objectives = []\n",
    "    for X, label in zip(Xs, labels):\n",
    "        maxiter = X.shape[0]\n",
    "        objective = np.array([f(A, X[i,:], y) for i in range(maxiter)])\n",
    "        objective -= f(A, x_natural, y)\n",
    "        objectives.append(objective)\n",
    "        plt.semilogy(np.arange(maxiter)+1, objective, label=label)\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('convergence')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('function sub-optimality')\n",
    "    return objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_no  = lambda A, x, y: np.sum((A.dot(x) - y)**2) / 2\n",
    "gradf = lambda A, x, y, i: A[i,:].T.dot(A[i,:].dot(x) - y[i])\n",
    "prox_no = lambda x, gamma: x\n",
    "\n",
    "n = 100\n",
    "A = np.identity(n)\n",
    "y = np.arange(n)\n",
    "\n",
    "params = {'maxiter': 400, 'gamma': 0.1}\n",
    "\n",
    "tstart = time.process_time()\n",
    "x = saga_lstsq_authors(A, y, **params)\n",
    "print('Elapsed time: {:.2f}s'.format(time.process_time() - tstart))\n",
    "np.testing.assert_allclose(x, y)\n",
    "\n",
    "tstart = time.process_time()\n",
    "X1 = saga(A, y, gradf, prox_no, **params)\n",
    "print('Elapsed time: {:.2f}s'.format(time.process_time() - tstart))\n",
    "np.testing.assert_allclose(X1[-1,:], y)\n",
    "\n",
    "tstart = time.process_time()\n",
    "X2 = saga_mb(A, y, gradf, prox_no, **params, mb_size=10)\n",
    "print('Elapsed time: {:.2f}s'.format(time.process_time() - tstart))\n",
    "np.testing.assert_allclose(X2[-1,:], y)\n",
    "\n",
    "tstart = time.process_time()\n",
    "X3 = saga_mb_2(A, y, gradf, prox_no, **params, mb_size=10)\n",
    "print('Elapsed time: {:.2f}s'.format(time.process_time() - tstart))\n",
    "np.testing.assert_allclose(X3[-1,:], y)\n",
    "\n",
    "plot_convergence(f_no, A, y, [X1, X2, X3], y, ['SAGA', 'SAGA, fixed mb', 'SAGA, mb per epoch']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression: SAGA\n",
    "\n",
    "Compare our various implementations of SAGA and be sure that they yield the same results.\n",
    "\n",
    "* They all get the same performance with the same learning rate.\n",
    "* However, bigger the mini-batch size, smaller the learning rate to avoid divergence.\n",
    "* At similar running time, the mini-batch method always beats the standard one (in R^2 score and loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxiter = 20\n",
    "maxiter_mb = 400\n",
    "gamma = 5e-9\n",
    "gamma_mb = 4e-10 # 2e-9 for 100, 4e-10 for 500\n",
    "mb_size = 500\n",
    "\n",
    "tstart = time.process_time()\n",
    "x = saga_lstsq_authors(train_data, train_target, maxiter, gamma)\n",
    "print('R^2 score: {:.4f} --> {:.4f} ({:.2f}s)'.format(\n",
    "        score(x, 'train'), score(x, 'test'), time.process_time()-tstart))\n",
    "\n",
    "tstart = time.process_time()\n",
    "X1 = saga(train_data, train_target, gradf, prox_no, maxiter, gamma)\n",
    "print('R^2 score: {:.4f} --> {:.4f} ({:.2f}s)'.format(\n",
    "        score(X1[-1,:], 'train'), score(X1[-1,:], 'test'), time.process_time()-tstart))\n",
    "\n",
    "tstart = time.process_time()\n",
    "X2 = saga_mb(train_data, train_target, gradf, prox_no, maxiter_mb, gamma_mb, mb_size)\n",
    "print('R^2 score: {:.4f} --> {:.4f} ({:.2f}s)'.format(\n",
    "        score(X2[-1,:], 'train'), score(X2[-1,:], 'test'), time.process_time()-tstart))\n",
    "\n",
    "tstart = time.process_time()\n",
    "X3 = saga_mb_2(train_data, train_target, gradf, prox_no, maxiter_mb, gamma_mb, mb_size)\n",
    "print('R^2 score: {:.4f} --> {:.4f} ({:.2f}s)'.format(\n",
    "        score(X3[-1,:], 'train'), score(X3[-1,:], 'test'), time.process_time()-tstart))\n",
    "\n",
    "plot_convergence(f_no, train_data, train_target, [X1, X2, X3], x_natural, ['regular', 'fixed mb', 'mb per epoch']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxiter = 20\n",
    "gamma = 5e-9\n",
    "\n",
    "tstart = time.process_time()\n",
    "X1 = saga(train_data, train_target, gradf, prox_no, maxiter, gamma)\n",
    "print('R^2 score: {:.4f} --> {:.4f} ({:.2f}s)'.format(\n",
    "        score(X1[-1,:], 'train'), score(X1[-1,:], 'test'), time.process_time()-tstart))\n",
    "\n",
    "reg = 1e3\n",
    "f_l2  = lambda A, x, y: np.sum((A.dot(x) - y)**2) / 2 + reg * np.linalg.norm(x, 2)\n",
    "prox_l2 = lambda x, gamma: 1 / (1+reg*gamma) * x;\n",
    "\n",
    "tstart = time.process_time()\n",
    "X2 = saga(train_data, train_target, gradf, prox_l2, maxiter, gamma)\n",
    "print('R^2 score: {:.4f} --> {:.4f} ({:.2f}s)'.format(\n",
    "        score(X2[-1,:], 'train'), score(X2[-1,:], 'test'), time.process_time()-tstart))\n",
    "\n",
    "reg = 1e0\n",
    "f_l1  = lambda A, x, y: np.sum((A.dot(x) - y)**2) / 2 + reg * np.linalg.norm(x, 1)\n",
    "prox_l1 = lambda x, gamma: np.sign(x) * np.maximum(np.abs(x) - gamma*reg, 0)\n",
    "\n",
    "tstart = time.process_time()\n",
    "X3 = saga(train_data, train_target, gradf, prox_l1, maxiter, gamma)\n",
    "print('R^2 score: {:.4f} --> {:.4f} ({:.2f}s)'.format(\n",
    "        score(X3[-1,:], 'train'), score(X3[-1,:], 'test'), time.process_time()-tstart))\n",
    "\n",
    "plot_convergence(f_no, train_data, train_target, [X1, X2, X3], x_natural, ['no reg', 'l2 reg', 'l1 reg']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Measure different metrics with respect to the mini-batch size.\n",
    "\n",
    "* Performance: loss and R^2 score.\n",
    "* Speed: total running time.\n",
    "* Memory: MB used to store the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mb_sizes = [1, 20, 50, 100, 200, 500]\n",
    "gammas = [4e-9, 3e-9, 3e-9, 1e-9, 1e-9, 4e-10]\n",
    "maxiters = [12, 170, 314, 440, 540, 380]\n",
    "\n",
    "Xs, train_scores, test_scores, ptimes = [], [], [], []\n",
    "for mb_size, gamma, maxiter in zip(mb_sizes, gammas, maxiters):\n",
    "    tstart = time.process_time()\n",
    "    X = saga_mb(train_data, train_target, gradf, prox_no, maxiter, gamma, mb_size)\n",
    "    ptimes.append(time.process_time() - tstart)\n",
    "    Xs.append(X)\n",
    "    train_scores.append(score(X[-1,:], 'train'))\n",
    "    test_scores.append(score(X[-1,:], 'test'))\n",
    "    print('size {:4d} in {:.2f}s'.format(mb_size, ptimes[-1]))\n",
    "\n",
    "labels = ['mini-batch size = {}'.format(mb_size) for mb_size in mb_sizes]\n",
    "objectives = plot_convergence(f_no, train_data, train_target, Xs, x_natural, labels)\n",
    "\n",
    "plt.figure(figsize=(17,5))\n",
    "for maxiter, label, ptime, objective in zip(maxiters, labels, ptimes, objectives):\n",
    "    tepoch = np.linspace(0, ptime, maxiter)\n",
    "    plt.semilogy(tepoch, objective, '-', label=label)\n",
    "plt.legend(loc='best')\n",
    "plt.xlim(0, min(ptimes))\n",
    "plt.title('Performance')\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig('performance.pdf')\n",
    "\n",
    "plt.figure(figsize=(17,5))\n",
    "plt.plot(mb_sizes, train_scores, '.-', label='training set')\n",
    "plt.plot(mb_sizes, test_scores, '.-', label='testing set')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Performance')\n",
    "plt.xlabel('mini-batch size')\n",
    "plt.ylabel('R^2 score')\n",
    "plt.savefig('r2_score.pdf')\n",
    "\n",
    "plt.figure(figsize=(17,5))\n",
    "plt.plot(mb_sizes, ptimes, '.-')\n",
    "plt.title('Processing time')\n",
    "plt.xlabel('mini-batch size')\n",
    "plt.ylabel('time [s]')\n",
    "\n",
    "plt.figure(figsize=(17,5))\n",
    "memory = 8 * train_data.size / np.array(mb_sizes) / 1024**2\n",
    "plt.semilogy(mb_sizes, memory, '.-')\n",
    "plt.title('Memory to store gradients')\n",
    "plt.xlabel('mini-batch size')\n",
    "plt.ylabel('working memory [MB]');\n",
    "plt.savefig('memory.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
