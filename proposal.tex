\documentclass[a4paper,11pt]{article}

\usepackage[T1]{fontenc} \usepackage{lmodern} \usepackage[utf8]{inputenc}
\usepackage[english]{babel} \usepackage{csquotes}
\usepackage{float} \usepackage{graphicx,subfig}
\usepackage{amssymb,amsmath} \usepackage{siunitx}
\usepackage[nodayofweek]{datetime}
\usepackage[top=3.5cm,bottom=2.5cm,left=3cm,right=3cm,headheight=30pt]{geometry}
%\usepackage[style=alphabetic,backend=biber]{biblatex} \bibliography{refs}
\usepackage{fancyhdr} \pagestyle{fancy} \usepackage{lastpage}
%\usepackage{parskip} \setlength{\parskip}{1em} %\setlength{\parindent}{0em}
\usepackage[colorlinks=true,allcolors=blue]{hyperref} \hypersetup{
	pdfauthor={Michaël Defferrard, Soroosh Shafiee},
	pdftitle={Incremental Gradient Methods},
	pdfsubject={Project proposal}
}
\lhead{Advanced Topics in Data Sciences\\ Project proposal}
\chead{\hspace{2cm}EPFL\\ \hspace{2cm}\shortdate\today}
\rhead{Michaël \textsc{Defferrard}\\ Soroosh \textsc{Shafiee}}
\cfoot{}

\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\R}{\mathbb{R}}
\newcommand{\eqnref}[1]{(\ref{eqn:#1})}

\begin{document}

\begin{center} 
	\Large{\textbf{\textsc{Incremental Gradient Methods}}}
\end{center}

%\section{Description}

This project is aimed to be a way for us to better understand and thinker with
the recent advances in the Stochastic Gradient Descent algorithms, specifically
some of the newest Incremental Gradient Methods such as SAG, SVRG or SAGA. This
class of algorithms have been developed to solve problems of the form
\begin{equation} \label{eqn:problem}
	\argmin_{x \in \R^d} \frac{1}{n} \sum_{i=1}^n f_i(x) + h(x),
\end{equation}
where each $f_i$ is convex and has Libschitz continuous derivatives with
constant $L$ or is strongly convex with constant $\mu$. The regularization
function $h$ is convex but potentially non-differentiable (his proximal operator
is however easy to compute). These stochastic algorithms reduce the
computational cost by only computing the gradient of a subset of the functions,
which would otherwise be prohibitive due to large $d$ and $n$

Many machine learning problems can be cast in \eqnref{problem}, such as the
Least-Square or Logistic Regressions with $\ell_2$ or $\ell_1$ regularization;
where $x$ would represent the model parameters and $f_i$ the data fidelity term
applied to a particular sample $i$. As such, these methods are of use in our
respective domains of expertise: Signal Processing on Graphs and Risk Analytics.

With the general setting in mind, we identify four directions relevant to our
research in which we could contribute:
\begin{enumerate}
	%\setlength{\itemsep}{0pt} \setlength{\parskip}{0pt}
	\item Play with the trade-off between the computational efficiency of SAGA
		and the memory efficiency of SVRG, especially relevant when working with
		large datasets. A first approach would be to store averaged gradients
		over mini-batches instead of the full gradient matrix. This task will
		involve the implementation and empirical testing of the devised scheme.
		A novel proof of convergence can be envisioned.
	\item A distributed implementation of one of those algorithms. This would be
		useful to diminish the clock time needed to solve a given problem or to
		solve large scale optimizations where the memory of one computer is not
		sufficient anymore. This goal will require the analysis of the
		inter-nodes communication cost as well as the design of a merging or
		synchronization scheme. Novel proofs of convergence could be required.
	\item Explore the application of these algorithms to minmax problems which
		aim at finding saddle points.
	\item Use these methods to fit various statistical models.
		%manifold optimization problems (e.g. GMM).
\end{enumerate}

We do not expect to complete all of the above objectives. While we are confident
about the success of the first two, we are not sure about the feasibility of the
last two. We plan to discuss with experts in the domain\footnote{Such as Reshad
Hosseini, which Soroosh met during his master studies}.

%\section{Plan}
%\section{Feasibility}
%\section{Roles}
%Michaël: 1 and 

\end{document}
