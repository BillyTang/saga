\documentclass[a4paper,11pt]{article}

\usepackage[T1]{fontenc} \usepackage{lmodern} \usepackage[utf8]{inputenc}
\usepackage[english]{babel} \usepackage{csquotes}
\usepackage{float} \usepackage{graphicx,subfig}
\usepackage{amssymb,amsmath} %\usepackage{siunitx}
\usepackage[nodayofweek]{datetime}
\usepackage[top=3.5cm,bottom=2.5cm,left=3cm,right=3cm,headheight=30pt]{geometry}
\usepackage[style=numeric,backend=biber]{biblatex} \bibliography{refs}
\usepackage{fancyhdr} \pagestyle{fancy} \usepackage{lastpage}
\usepackage{parskip} \setlength{\parskip}{.5em} \setlength{\parindent}{1em}
\usepackage[colorlinks=true,allcolors=blue]{hyperref} \hypersetup{
	pdfauthor={Michaël Defferrard, Soroosh Shafiee},
	pdftitle={Incremental Gradient Methods},
	pdfsubject={Project proposal}
}
\lhead{Advanced Topics in Data Sciences\\ Project proposal}
\chead{\hspace{2cm}EPFL\\ \hspace{2cm}\shortdate\today}
\rhead{Michaël \textsc{Defferrard}\\ Soroosh \textsc{Shafiee}}
\cfoot{}

\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\R}{\mathbb{R}}
\newcommand{\eqnref}[1]{(\ref{eqn:#1})}

\begin{document}

\begin{center} 
	\Large{\textbf{\textsc{Incremental Gradient Methods}}}
\end{center}

This project is aimed to be a way for us to better understand and thinker with
the recent advances in the Stochastic Gradient Descent algorithms, specifically
some of the newest Incremental Gradient Methods such as SAG
\cite{schmidt_minimizing_2013}, SVRG \cite{johnson_accelerating_2013} or SAGA
\cite{defazio_saga_2014}. This class of algorithms have been developed to solve
problems of the form
\begin{equation} \label{eqn:problem}
	\min_{x \in \R^d} \frac{1}{n} \sum_{i=1}^n f_i(x) + h(x),
\end{equation}
where each $f_i$ is convex and has Libschitz continuous derivatives with
constant $L$ or is strongly convex with constant $\mu$. The function $h$ can be seen as a regularizer which is convex and potentially non-differentiable (his proximal operator
is however easy to compute) or it can be seen as a characteristic function of a set to tackle constrained problems. While computing the full gradient would be
prohibitive due to large $d$ and $n$, these iterative stochastic algorithms
reduce the computational cost of optimization by only computing the gradient of
a subset of the functions $f_i$ at each step.

Many machine learning problems can be cast in \eqnref{problem}, such as the
Least-Square or Logistic Regressions with $\ell_2$ or $\ell_1$ regularization;
where $x$ would represent the model parameters and $f_i$ the data fidelity term
applied to a particular sample $i$. As such, these methods are of use in our
respective domains of expertise: Signal Processing on Graphs and Risk Analytics.

With the general setting in mind, we identify four directions relevant to our
research in which we could contribute:
\begin{enumerate}
	\setlength{\itemsep}{0pt} \setlength{\parskip}{0pt}
	\item Play with the trade-off between the computational efficiency of SAGA
		and the memory efficiency of SVRG, especially relevant when working with
		large datasets, e.g. covtype.scale dataset with $n=522910$. A first approach to compromise on the
		memory requirement of SAGA would be to store averaged gradients over
		mini-batches instead of the full gradient matrix. This task will involve
		the implementation and empirical testing of the devised scheme such as the method proposed in \cite{nitanda2014stochastic}. A novel
		proof of convergence can be envisioned.
	\item A distributed implementation of one of those algorithms. This would be
		useful to diminish the clock time needed to solve a given problem or to
		solve large-scale optimizations where the memory of one computer is not
		sufficient anymore. This goal will require the analysis of the
		inter-nodes communication cost as well as the design of a merging or
		synchronization scheme \cite{bianchi2014coordinate}. Novel proofs of convergence could be required.
	\item Explore the application of these algorithms to minimax problems which
		aim at finding saddle points \cite{nemirovski2009robust}. The min-max formulation appears in the
		context of zero-sum games and robust optimization. Traditionally, robust
		optimization problems focus on converting the minimax problem to a
		minimization problem by leveraging duality theory. Instead, we aim to
		find the saddle points using incremental methods.
	\item Use these methods to fit statistical models. In particular, we are
		interested to fit a Gaussian Mixture Model (GMM) viewed as a manifold
		optimization problem. Our goal would be to adapt one of the incremental
		methods to fit GMMs \cite{reshad_matrix_2015}.
\end{enumerate}
We do not expect to complete all of the above objectives. We plan to discuss with experts in the domain\footnote{Such as the
first author of \cite{reshad_matrix_2015}, whom Soroosh met during his master
studies. Or someone from the EPFL LIONS lab.}.

\paragraph{Roles.}
Each of us will pursue one of the mentioned goals from beginning to end; which
includes any necessary theory, implementation, testing, writing and
presentation. Our work (code, report and presentation) will be tracked by
\textit{git}, such that individual contributions can easily be spotted.

\paragraph{Milestones.}
Following are the milestones we envision for the completion of the
aforementioned project.
\begin{itemize}
	\setlength{\itemsep}{0pt} \setlength{\parskip}{0pt}
	\item 2016-03-24 Proposal submitted.
	\item 2016-04-01 Proposal approved.
	\item 2016-04-08 Two directions chosen.
	\item 2016-04-22 Problems stated and solutions formulated.
	\item 2016-05-06 Solutions implemented (Jupyter notebooks, Python).
	\item 2016-05-20 Tested on real or synthetic data.
	\item 2016-05-27 Report written.
	\item 2016-06-03 Project presented.
\end{itemize}

\printbibliography

\end{document}
