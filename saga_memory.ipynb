{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory-efficient SAGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyximport; pyximport.install()\n",
    "import saga_authors\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "import sklearn.linear_model, sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Problem: Prediction of the release year of a song from audio features. Songs are mostly western, commercial tracks ranging from 1922 to 2011, with a peak in the year 2000s.\n",
    "\n",
    "* Dataset can be downloaded at https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD.\n",
    "* 90 audio attributes: 12 = timbre average, 78 = timbre covariance\n",
    "    * Features extracted from the 'timbre' features from The Echo Nest API. We take the average and covariance over all 'segments', each segment being described by a 12-dimensional timbre vector.\n",
    "* The first value is the year (target), ranging from 1922 to 2011. \n",
    "* train: first 463,715 examples, test: last 51,630 examples\n",
    "    * It avoids the 'producer effect' by making sure no song from a given artist ends up in both the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('YearPredictionMSD.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Song release year.\n",
    "train_target = data[:463715,0]  #.astype(np.int32)\n",
    "test_target  = data[463715:,0]  #.astype(np.int32)\n",
    "assert test_target.shape == (51630,)\n",
    "\n",
    "# Audio features.\n",
    "train_data = data[:463715,1:]\n",
    "test_data  = data[463715:,1:]\n",
    "assert train_data.shape == (463715,90)\n",
    "assert test_data.shape == (51630,90)\n",
    "\n",
    "# Subset for fast testing purpose.\n",
    "#n = 10000\n",
    "#train_data = data[:n,1:]\n",
    "#train_target = data[:n,0]\n",
    "\n",
    "# Normalize so as to do not need an intercept.\n",
    "intercept = train_target.mean()\n",
    "data_mean = train_data.mean(axis=0)\n",
    "train_target = train_target - intercept\n",
    "test_target = test_target - intercept\n",
    "train_data = train_data - data_mean\n",
    "test_data = test_data - data_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression: ground truth\n",
    "\n",
    "* Sanity check of the performance of a linear classifier.\n",
    "* Exact least-square solution to compare with SAGA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_natural = np.linalg.lstsq(train_data, train_target)[0]\n",
    "\n",
    "cls = sklearn.linear_model.LinearRegression(fit_intercept=True)\n",
    "cls.fit(train_data, train_target)\n",
    "assert cls.intercept_ < 1e-10\n",
    "np.testing.assert_allclose(cls.coef_, x_natural)\n",
    "\n",
    "def score(x, dataset, plot=2000):\n",
    "    A = eval('{}_data'.format(dataset))\n",
    "    y = eval('{}_target'.format(dataset)) + intercept\n",
    "    pred = A.dot(x) + intercept\n",
    "    score = sklearn.metrics.r2_score(y, pred)\n",
    "    print('R^2 score on {} set: {:.4f}'.format(dataset, score))\n",
    "    \n",
    "    if plot > 0:\n",
    "        plt.figure(figsize=(17,5))\n",
    "        plt.plot(pred[:plot], '.', label='predicted')\n",
    "        plt.plot(y[:plot], '.', label='ground truth')\n",
    "        plt.title(dataset)\n",
    "        plt.xlabel('sample')\n",
    "        plt.ylabel('release year')\n",
    "        plt.legend()\n",
    "score(x_natural, 'train')\n",
    "score(x_natural, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAGA\n",
    "\n",
    "* Cython code from the authors.\n",
    "    * To solve least-square problems only.\n",
    "    * It uses the tricks from Section 4 of the paper.\n",
    "* Minimal and straightforward implementation by us.\n",
    "    * It provides the same results and is only 3 times slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def saga_lstsq_authors(A, y, maxiter, gamma, reg=0):\n",
    "    \"\"\"\n",
    "    Solve min_x ||Ax - b||_2^2 reg*||x||_2^2\n",
    "    \n",
    "    Parameters:\n",
    "        gamma: step size or learning rate\n",
    "        reg:   amount of L2 regularization\n",
    "    \"\"\"\n",
    "    tstart = time.process_time()\n",
    "    \n",
    "    # Proper data type.\n",
    "    A = sparse.csc_matrix(A.T)\n",
    "    A.indices = A.indices.astype(np.int64)\n",
    "    A.indptr = A.indptr.astype(np.int64)\n",
    "    y = y.astype(np.float64)\n",
    "    \n",
    "    # Algorithm (eta is the inverse step size).\n",
    "    props = {'eta': 1 / gamma, 'reg': reg}\n",
    "    x = saga_authors.saga_lstsq(A, y, maxiter, props)\n",
    "    \n",
    "    print('Elapsed time: {:.2f}s'.format(time.process_time() - tstart))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def saga(A, y, gradf, prox, maxiter, gamma):\n",
    "    n, d = A.shape  # number of observations x dimensionality\n",
    "    x = np.zeros(d)\n",
    "    grads = np.array([gradf(x,i) for i in range(n)])\n",
    "    avg = grads.mean(axis=0)\n",
    "    X = np.empty((maxiter, d))\n",
    "    \n",
    "    for epoch in range(maxiter):\n",
    "        #indices = np.random.permutation(n)\n",
    "        indices = np.random.randint(0, n, n)\n",
    "        \n",
    "        for i in indices:\n",
    "            grad = gradf(x,i)\n",
    "            x = x - gamma * (grad - grads[i,:] + avg)\n",
    "            x = prox(x, gamma)\n",
    "            avg += (grad - grads[i,:]) / n\n",
    "            grads[i,:] = grad\n",
    "            \n",
    "        X[epoch,:] = x\n",
    "        \n",
    "    return X\n",
    "\n",
    "def saga_lstsq(A, y, maxiter, gamma, reg=0, x_natural=None):\n",
    "    tstart = time.process_time()\n",
    "    \n",
    "    gradf = lambda x, i: A[i,:].T.dot(A[i,:].dot(x) - y[i])\n",
    "    prox  = lambda x, gamma: x\n",
    "    \n",
    "    X = saga(A, y, gradf, prox, maxiter, gamma)\n",
    "    print('Elapsed time: {:.2f}s'.format(time.process_time() - tstart))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_convergence(A, y, X, x_natural):\n",
    "    f = lambda x: np.sum((A.dot(x) - y)**2) / 2\n",
    "    maxiter = X.shape[0]\n",
    "    objective = np.array([f(X[i,:]) for i in range(maxiter)])\n",
    "    objective -= f(x_natural)\n",
    "    \n",
    "    plt.figure(figsize=(17,5))\n",
    "    plt.semilogy(np.arange(maxiter)+1, objective)\n",
    "    plt.xlim(1, maxiter)\n",
    "    plt.title('convergence')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('function sub-optimality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "A = np.identity(n)\n",
    "y = np.arange(n)\n",
    "\n",
    "x = saga_lstsq_authors(A, y, maxiter=400, gamma=0.1, reg=0)\n",
    "np.testing.assert_allclose(x, y)\n",
    "\n",
    "X = saga_lstsq(A, y, maxiter=400, gamma=0.1, reg=0)\n",
    "np.testing.assert_allclose(X[-1,:], y)\n",
    "plot_convergence(A, y, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression: SAGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {'maxiter': 100, 'gamma': 1e-9, 'reg': 0}\n",
    "\n",
    "x = saga_lstsq_authors(train_data, train_target, **params)\n",
    "score(x, 'train', plot=0)\n",
    "score(x, 'test', plot=0)\n",
    "\n",
    "X = saga_lstsq(train_data, train_target, **params)\n",
    "plot_convergence(train_data, train_target, X, x_natural)\n",
    "score(X[-1,:], 'train')\n",
    "score(X[-1,:], 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
